{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6e25c3-368b-48f9-be78-838f6fab11ca",
   "metadata": {},
   "source": [
    "### Global Variables\n",
    "\n",
    "#### make sure to change the global variables as per your requirements!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6528f2ba-9bc2-4baa-8361-ebc141d7b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KF_SERVING_COMPONENT = \"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/kfserving/component.yaml\"\n",
    "\n",
    "NAMESPACE = \"kubeflow-user-example-com\"\n",
    "BUCKET_NAME = \"pranava-data\"\n",
    "EXPERIMENT_NAME = \"Hello-World\"\n",
    "MODEL_NAME = \"iris1\"\n",
    "\n",
    "BASE_IMAGE = \"mesosphere/kubeflow:1.2.0-1.1.0-tensorflow-2.4.0\"\n",
    "\n",
    "COOKIE = \"MTY0NjM2NTQwMXxOd3dBTkZrMFFrdFZRakkxUjBaSU1sUTNORVZLVkVRMlZrSkpNa3ROVFVoQk4wdEVOa1kyTjBOSlJESkRTVFpRUjBoRldGRTFSVkU9fHI5F0-aM6Ld33E7yLEYFJ6SW9CHm90PaFteXkRRNZU7\"\n",
    "\n",
    "# DATASET_URL = \"https://www.kaggle.com/uciml/iris/download\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888ecb2-fd99-4c76-94c7-45519d6d56e1",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e91ce88-47a4-47a5-9cfd-0d43baef5442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "import kfp.components as components\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd489bac-a201-489a-aac9-974135704e97",
   "metadata": {},
   "source": [
    "### Building Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7260b37-2a1f-4c40-9c59-b5516da03680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(input_bucket: str) -> NamedTuple(\"Preprocess\", [('features', str),('target', str)]):\n",
    "    \"\"\"\n",
    "    downloads data from s3 and preprocess the data\n",
    "    and saves the processed data to s3\n",
    "    :param input_bucket: Bucket name where the data is present\n",
    "    :return: returns the preprocessed data file names that are stored in s3\n",
    "    \"\"\"\n",
    "    \n",
    "    import boto3\n",
    "    import pandas as pd\n",
    "    from io import StringIO\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    PreProcessedData = namedtuple(\"Preprocess\", ['features','target'])\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file(input_bucket, \"Iris.csv\", \"Iris.csv\")\n",
    "    \n",
    "    def save_data(data, filename):\n",
    "        \"\"\"\n",
    "        uploads dataframes to s3\n",
    "        :param data: the data to be written to s3\n",
    "        :param filename: the name of the file where the data should be stored\n",
    "        \"\"\"\n",
    "        \n",
    "        csv_buffer = StringIO()\n",
    "        data.to_csv(csv_buffer)\n",
    "        s3_resource = boto3.resource('s3')\n",
    "        s3_resource.Object(input_bucket, filename).put(Body=csv_buffer.getvalue())\n",
    "    \n",
    "    \n",
    "    iris = pd.read_csv('Iris.csv')\n",
    "    \n",
    "    iris.drop('Id',axis=1,inplace=True)\n",
    "    \n",
    "    x = iris.drop(columns='Species')\n",
    "    y = iris['Species']\n",
    "    \n",
    "    save_data(x,\"features.csv\")\n",
    "    save_data(y,\"target.csv\")\n",
    "    \n",
    "    return PreProcessedData(\"features.csv\", \"target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c8f3418-57f7-455f-b211-3075a9e217a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(preprocess:dict, input_bucket:str) -> str:\n",
    "    \"\"\"\n",
    "    downloads the features and target data from s3\n",
    "    trains the model and uploads to s3\n",
    "    :param preprocess: a dictionary containing filenames of preprocessed data\n",
    "    :param input_bucket: Bucket name where the data is present\n",
    "    :return: returns name of the model that is saved to s3\n",
    "    \"\"\"\n",
    "    \n",
    "    import boto3\n",
    "    import pandas as pd\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn import tree\n",
    "    import joblib\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    def get_data(key:str):\n",
    "        \"\"\"\n",
    "        downloads data from s3\n",
    "        :param key: key for the filename\n",
    "        :return: returns the data in a dataframe format\n",
    "        \"\"\"\n",
    "        filename = preprocess[key]\n",
    "        s3.download_file(\n",
    "            input_bucket,\n",
    "            filename, \n",
    "            filename\n",
    "        )  \n",
    "        return pd.read_csv(filename).iloc[:, 1:]\n",
    "    \n",
    "    classifier = DecisionTreeClassifier()\n",
    "    \n",
    "    x = get_data(\"features\")\n",
    "    y = get_data(\"target\")\n",
    "    \n",
    "    classifier.fit(x,y)\n",
    "    \n",
    "    model_name = \"model\"\n",
    "    \n",
    "    joblib.dump(classifier, model_name)\n",
    "    \n",
    "    s3.upload_file(model_name, input_bucket, model_name)\n",
    "    \n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "530aa2fa-0ee1-4c9a-9ec6-b40bcdfc976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(bucket: str, model_name: str):\n",
    "    \"\"\"\n",
    "    loads the model and displays the predicted results\n",
    "    :param input_path: location of model file\n",
    "    :param model_name: name of the model\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    import boto3\n",
    "    import joblib\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file(bucket, model_name, model_name)\n",
    "    \n",
    "    model = joblib.load(model_name)\n",
    "    \n",
    "    print(model.predict([[\"5.1\",\"3.5\", \"1.4\", \"0.2\"]]))\n",
    "    print(model.predict([[\"6.4\", \"3.2\", \"4.5\", \"1.5\"]]))\n",
    "    print(model.predict([[\"6.7\",\"3.3\", \"5.7\", \"2.5\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "073717a6-b27d-479f-98a9-c23a5c2f0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(model_volume_op, model_name: str) -> str:\n",
    "    \"\"\"\n",
    "    downloads the model from s3 and attaches to a Persistent Volume\n",
    "    :param model_name: name of the model\n",
    "    :return: returns a success message if the model is downloaded, throws an error otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    import boto3\n",
    "    import os\n",
    "    \n",
    "    s3 = boto3.client(\"s3\")\n",
    "    \n",
    "    if os.path.exists(\"/mnt/e2e\"):\n",
    "        if not os.path.exists(\"/mnt/e2e/iris\"):\n",
    "            os.mkdir(\"/mnt/e2e/iris\")\n",
    "        s3.download_file(\n",
    "            \"pranava-data\",\n",
    "            model_name, \n",
    "            \"/mnt/e2e/iris/model.joblib\"\n",
    "        )        \n",
    "    else:\n",
    "        raise Exception(\"Volume not Mounted\")\n",
    "    return \"downloaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72bff923-8771-4e8d-ad50-880d678527de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kfserving_task(model_name, model_namespace, tfjob_op, model_volume_op):\n",
    "    \"\"\"\n",
    "    creates a kfserve inference\n",
    "    :param model_name: name of the model\n",
    "    :param model_namespace: namespace\n",
    "    \"\"\"\n",
    "    inference_service = '''\n",
    "apiVersion: \"serving.kubeflow.org/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: {}\n",
    "  namespace: {}\n",
    "  annotations:\n",
    "    \"sidecar.istio.io/inject\": \"false\"\n",
    "spec:\n",
    "  predictor:\n",
    "    sklearn:\n",
    "      storageUri: \"pvc://{}/iris\"\n",
    "'''.format(model_name, model_namespace, str(model_volume_op.outputs[\"name\"]))\n",
    "    \n",
    "    return kfservingLauncherOP(action=\"create\", inferenceservice_yaml=inference_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1899a19-9a7b-4405-8969-4897e848d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfserve_predict(model_name: str, namespace: str, cookie: str):\n",
    "    \"\"\"\n",
    "    predicts the model using kfserve generated internal api\n",
    "    :param model_name: name of the model\n",
    "    :param namespace: namespace where the run should be executed\n",
    "    :param cookie: to authenticate\n",
    "    \"\"\"\n",
    "    \n",
    "    import requests\n",
    "    import json\n",
    "    \n",
    "    authservice_session={'authservice_session': cookie}\n",
    "    url = \"http://{}.{}.svc.cluster.local/v1/models/{}:predict\".format(\n",
    "        model_name, \n",
    "        namespace, \n",
    "        model_name\n",
    "    )\n",
    "    data = {\n",
    "      \"instances\": [\n",
    "        [\"5.1\",\"3.5\", \"1.4\", \"0.2\"],\n",
    "        [\"6.4\", \"3.2\", \"4.5\", \"1.5\"],\n",
    "        [\"6.7\",\"3.3\", \"5.7\", \"2.5\"]\n",
    "      ]\n",
    "    }\n",
    "\n",
    "\n",
    "    res = requests.post(\n",
    "        url,\n",
    "        json=data,\n",
    "        cookies=authservice_session\n",
    "    )\n",
    "    print(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87c142f-818c-42af-8676-96721bfe8adb",
   "metadata": {},
   "source": [
    "### creating components from the above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22f7e429-b434-443d-a8e7-a7ea5e81f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPreprocessOP = components.func_to_container_op(data_preprocess, base_image=BASE_IMAGE)\n",
    "\n",
    "modelTrainOP = components.func_to_container_op(train_model, base_image=BASE_IMAGE)\n",
    "\n",
    "predictModelOP = components.func_to_container_op(predict_model, base_image=BASE_IMAGE)\n",
    "\n",
    "downloadOP = components.func_to_container_op(download_file, base_image=BASE_IMAGE)\n",
    "\n",
    "kfservingLauncherOP = components.load_component_from_url(KF_SERVING_COMPONENT)\n",
    "\n",
    "kfservePredictionOP = components.func_to_container_op(kfserve_predict,base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dcadf5-023e-4c4c-8721-347f519e5f26",
   "metadata": {},
   "source": [
    "### defining the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37f32745-df51-4eb4-8c62-1550e622da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Sample Hello world pipeline\",\n",
    "    description=\"A sample pipeline to demonstrate multi-step model training, evaluation, export using Iris data classification\",\n",
    ")\n",
    "def sample_pipeline(\n",
    "    input_bucket: str = BUCKET_NAME, \n",
    "    model_name: str = MODEL_NAME, \n",
    "    namespace: str = NAMESPACE\n",
    "):\n",
    "    \"\"\"\n",
    "    connects the components for a pipeline\n",
    "    :param input_bucket: Bucket name where the data is present\n",
    "    :param model_name: Name of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    preProcess = dataPreprocessOP(input_bucket)\n",
    "    \n",
    "    train = modelTrainOP(preProcess.outputs, input_bucket)\n",
    "    \n",
    "    predict = predictModelOP(input_bucket, train.output)\n",
    "    \n",
    "    model_volume_op = dsl.VolumeOp(\n",
    "        name=\"model-volume-demo-lscd\",\n",
    "        resource_name=\"model-volume-demo-mock-lscd\",\n",
    "        size=\"1Gi\",\n",
    "        modes=dsl.VOLUME_MODE_RWO\n",
    "    )\n",
    "    \n",
    "    tfjob_op = downloadOP(\"\", train.output).add_pvolumes(\n",
    "        {\"/mnt/e2e\":model_volume_op.volume })\n",
    "    tfjob_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        \n",
    "    kf_op = create_kfserving_task(\n",
    "        model_name, \n",
    "        namespace, \n",
    "        tfjob_op, \n",
    "        model_volume_op\n",
    "    )\n",
    "    kf_op.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    kf_op.after(tfjob_op)\n",
    "    \n",
    "    kfserve_predict = kfservePredictionOP(\n",
    "        model_name,\n",
    "        namespace,\n",
    "        COOKIE\n",
    "    )\n",
    "    kfserve_predict.after(kf_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61000d6a-a277-4b62-8326-03732deccea6",
   "metadata": {},
   "source": [
    "### connect to kfp client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af6aa8b3-6524-4142-b843-fe8dd1f7599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f7d2e-135b-4cf0-bc17-8a7bb66da0d4",
   "metadata": {},
   "source": [
    "### Create an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0a64e11-d72c-4acd-806b-8e97a605ea1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/de0e35d4-8d7f-4490-ba53-a2d2397a831e\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment = client.create_experiment(\n",
    "    name=EXPERIMENT_NAME, \n",
    "    description=\"A sample pipeline to demonstrate multi-step model training, evaluation, export using Iris data classification\",\n",
    "    namespace=NAMESPACE\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae77b98-8fbb-48f1-a9aa-cc92c312100c",
   "metadata": {},
   "source": [
    "### Create a Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "378ec6fc-940e-4a84-8203-0f9dcaea86d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/de0e35d4-8d7f-4490-ba53-a2d2397a831e\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/c6b32dfc-e184-40d2-a719-3203f3954eae\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=c6b32dfc-e184-40d2-a719-3203f3954eae)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_run_from_pipeline_func(\n",
    "    sample_pipeline, \n",
    "    arguments={}, \n",
    "    run_name=\"sample-iris-e2e\", \n",
    "    experiment_name=EXPERIMENT_NAME\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
